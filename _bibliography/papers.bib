---
---

@string{aps = {American Physical Society,}}

@book{KANOJIA202349,
title = {Chapter 3 - Applications and challenges of SA in real-life scenarios},
editor = {Dipankar Das and Anup Kumar Kolya and Abhishek Basu and Soham Sarkar},
booktitle = {Computational Intelligence Applications for Text and Sentiment Data Analysis},
publisher = {Academic Press},
pages = {49-80},
year = {2023},
series = {Hybrid Computational Intelligence for Pattern Analysis and Understanding},
isbn = {978-0-323-90535-0},
doi = {https://doi.org/10.1016/B978-0-32-390535-0.00008-2},
url = {https://www.sciencedirect.com/science/article/pii/B9780323905350000082},
author = {Diptesh Kanojia and Aditya Joshi},
keywords = {Sentiment analysis, Applications, Challenges, Real-life scenarios, Survey, Health, Social policy, Industry, Humanities},
abstract = {Sentiment analysis has benefited from the availability of lexicons and benchmark datasets created over decades of research. However, its applications to the real world are a driving force for research in SA. This chapter describes some of these applications and related challenges in real-life scenarios. In this chapter, we focus on five applications of SA: health, social policy, e-commerce, digital humanities, and other areas of NLP. This chapter is intended to equip an NLP researcher with the ‘what’, ‘why’, and ‘how’ of applications of SA: what is the application about, why it is important and challenging, and how current research in SA deals with the application. We note that, while the use of deep-learning techniques is a popular paradigm that spans these applications, challenges around privacy and selection bias of datasets is a recurring theme across several applications.}
}
@inproceedings{10.1145/3593013.3594134,
author = {Queerinai, Organizers Of and Ovalle, Anaelia and Subramonian, Arjun and Singh, Ashwin and Voelcker, Claas and Sutherland, Danica J. and Locatelli, Davide and Breznik, Eva and Klubicka, Filip and Yuan, Hang and J, Hetvi and Zhang, Huan and Shriram, Jaidev and Lehman, Kruno and Soldaini, Luca and Sap, Maarten and Deisenroth, Marc Peter and Pacheco, Maria Leonor and Ryskina, Maria and Mundt, Martin and Agarwal, Milind and Mclean, Nyx and Xu, Pan and Pranav, A and Korpan, Raj and Ray, Ruchira and Mathew, Sarah and Arora, Sarthak and John, St and Anand, Tanvi and Agrawal, Vishakha and Agnew, William and Long, Yanan and Wang, Zijie J. and Talat, Zeerak and Ghosh, Avijit and Dennler, Nathaniel and Noseworthy, Michael and Jha, Sharvani and Baylor, Emi and Joshi, Aditya and Bilenko, Natalia Y. and Mcnamara, Andrew and Gontijo-Lopes, Raphael and Markham, Alex and Dong, Evyn and Kay, Jackie and Saraswat, Manu and Vytla, Nikhil and Stark, Luke},
title = {Queer In AI: A Case Study in Community-Led Participatory AI},
year = {2023},
isbn = {9798400701924},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3593013.3594134},
doi = {10.1145/3593013.3594134},
abstract = {Queerness and queer people face an uncertain future in the face of ever more widely deployed and invasive artificial intelligence (AI). These technologies have caused numerous harms to queer people, including privacy violations, censoring and downranking queer content, exposing queer people and spaces to harassment by making them hypervisible, deadnaming and outing queer people. More broadly, they have violated core tenets of queerness by classifying and controlling queer identities. In response to this, the queer community in AI has organized Queer in AI, a global, decentralized, volunteer-run grassroots organization that employs intersectional and community-led participatory design to build an inclusive and equitable AI future. In this paper, we present Queer in AI as a case study for community-led participatory design in AI. We examine how participatory design and intersectional tenets started and shaped this community’s programs over the years. We discuss different challenges that emerged in the process, look at ways this organization has fallen short of operationalizing participatory and intersectional principles, and then assess the organization’s impact. Queer in AI provides important lessons and insights for practitioners and theorists of participatory methods broadly through its rejection of hierarchy in favor of decentralization, success at building aid and programs by and for the queer community, and effort to change actors and institutions outside of the queer community. Finally, we theorize how communities like Queer in AI contribute to the participatory design in AI more broadly by fostering cultures of participation in AI, welcoming and empowering marginalized participants, critiquing poor or exploitative participatory practices, and bringing participation to institutions outside of individual research projects. Queer in AI’s work serves as a case study of grassroots activism and participatory methods within AI, demonstrating the potential of community-led participatory methods and intersectional praxis, while also providing challenges, case studies, and nuanced insights to researchers developing and using participatory methods.},
booktitle = {Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency},
pages = {1882–1895},
numpages = {14},
location = {Chicago, IL, USA},
series = {FAccT '23}
}
@article{Joshi_Rawat_Dange_2023, 
  title={Evaluation of large language models using an Indian language LGBTI+ lexicon}, 
  volume={3}, 
  url={https://aiej.org/aiej/article/view/10}, 
  DOI={10.47289/AIEJ20231109}, 
  abstractNote={Large language models (LLMs) are typically evaluated on the basis of task-based benchmarks such as MMLU. Such benchmarks do not examine the behaviour of LLMs in specific contexts. This is particularly true in the LGBTI+ context where social stereotypes may result in variation in LGBTI+ terminology. Therefore, domain-specific lexicons or dictionaries may be useful as a representative list of words against which the LLM’s behaviour needs to be evaluated. This paper presents a methodology for evaluation of LLMs using an LGBTI+ lexicon in Indian languages. The methodology consists of four steps: formulating NLP tasks relevant to the expected behaviour, creating prompts that test LLMs, using the LLMs to obtain the output and, finally, manually evaluating the results. Our qualitative analysis shows that the three LLMs we experiment on are unable to detect underlying hateful content. Similarly, we observe limitations in using machine translation as means to evaluate natural language understanding in languages other than English. The methodology presented in this paper can be useful for LGBTI+ lexicons in other languages as well as other domain-specific lexicons. The work done in this paper opens avenues for responsible behaviour of LLMs in the Indian context, especially with prevalent social perception of the LGBTI+ community.}, 
  number={1}, 
  journal={The AI Ethics Journal}, 
  author={Joshi, Aditya and Rawat, Shruta and Dange, Alpana}, 
  year={2023}, 
  month={Nov.} 
}

@inproceedings{nguyen-etal-2023-stacking,
    title = {Stacking the Odds: Transformer-Based Ensemble for {AI}-Generated Text Detection},
    author = {Nguyen, Duke  and
      Naing, Khaing Myat Noe  and
      Joshi, Aditya},
    editor = {Muresan, Smaranda  and
      Chen, Vivian  and
      Casey, Kennington  and
      David, Vandyke  and
      Nina, Dethlefs  and
      Koji, Inoue  and
      Erik, Ekstedt  and
      Stefan, Ultes},
    booktitle = {Proceedings of the 21st Annual Workshop of the Australasian Language Technology Association},
    month = {nov},
    year = {2023},
    address = {Melbourne, Australia},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2023.alta-1.22},
    pages = {173--178},
    abstract = {This paper reports our submission under the team name {`}SynthDetectives{'} to the ALTA 2023 Shared Task. We use a stacking ensemble of Transformers for the task of AI-generated text detection. Our approach is novel in terms of its choice of models in that we use accessible and lightweight models in the ensemble. We show that ensembling the models results in an improved accuracy in comparison with using them individually. Our approach achieves an accuracy score of 0.9555 on the official test data provided by the shared task organisers.},
}

@article{hong2023relationextractionnewsarticles,
      title={Relation Extraction from News Articles (RENA): A Tool for Epidemic Surveillance}, 
      author={Jaeff Hong and Duong Dung and Danielle Hutchinson and Zubair Akhtar and Rosalie Chen and Rebecca Dawson and Aditya Joshi and Samsung Lim and C Raina MacIntyre and Deepti Gurdasani},
      year={2023},
      eprint={2311.01472},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2311.01472}, 
      type = {preprint}
}

@article{joshi2024naturallanguageprocessingdialects,
      title={Natural Language Processing for Dialects of a Language: A Survey}, 
      author={Aditya Joshi and Raj Dabre and Diptesh Kanojia and Zhuang Li and Haolan Zhan and Gholamreza Haffari and Doris Dippold},
      year={2024},
      eprint={2401.05632},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2401.05632}, 
      type = {preprint}
}

@article{srirag2024evaluatingdialectrobustnesslanguage,
      title={Evaluating Dialect Robustness of Language Models via Conversation Understanding}, 
      author={Dipankar Srirag and Nihar Ranjan Sahoo and Aditya Joshi},
      month = {May},
      year={2024},
      eprint={2405.05688},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2405.05688}, 
      type = {preprint}
}

@inproceedings{joshi-etal-2024-striking-balance,
    title = {Striking a Balance between Classical and Deep Learning Approaches in Natural Language Processing Pedagogy},
    author = {Joshi, Aditya  and
      Renzella, Jake  and
      Bhattacharyya, Pushpak  and
      Jha, Saurav  and
      Zhang, Xiangyu},
    editor = {Al-azzawi, Sana  and
      Biester, Laura  and
      Kov{\'a}cs, Gy{\"o}rgy  and
      Marasovi{\'c}, Ana  and
      Mathur, Leena  and
      Mieskes, Margot  and
      Weissweiler, Leonie},
    booktitle = {Proceedings of the Sixth Workshop on Teaching NLP},
    month = {aug},
    year = {2024},
    address = {Bangkok, Thailand},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2024.teachingnlp-1.4},
    pages = {23--32},
    abstract = {While deep learning approaches represent the state-of-the-art of natural language processing (NLP) today, classical algorithms and approaches still find a place in NLP textbooks and courses of recent years. This paper discusses the perspectives of conveners of two introductory NLP courses taught in Australia and India, and examines how classical and deep learning approaches can be balanced within the lecture plan and assessments of the courses. We also draw parallels with the objects-first and objects-later debate in CS1 education. We observe that teaching classical approaches adds value to student learning by building an intuitive understanding of NLP problems, potential solutions, and even deep learning models themselves. Despite classical approaches not being state-of-the-art, the paper makes a case for their inclusion in NLP courses today.},
}

@book{bhattacharyya2023natural,
  title={Natural Language Processing},
  author={Bhattacharyya, P. and Joshi, A.},
  isbn={9789357462389},
  series={Wiley emerging technology series},
  url={https://books.google.com.au/books?id=3bSY0AEACAAJ},
  year={2023}
}

@article{nguyen2024spectraformerunifiedrandomfeature,
      title={Spectraformer: A Unified Random Feature Framework for Transformer}, 
      author={Duke Nguyen and Aditya Joshi and Flora Salim},
      year={2024},
      eprint={2405.15310},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2405.15310}, 
      type = {preprint},
}

@inproceedings{shen-etal-2024-bambino,
    title = {{BAMBINO}-{LM}: (Bilingual-)Human-Inspired Continual Pre-training of {B}aby{LM}},
    author = {Shen, Zhewen  and
      Joshi, Aditya  and
      Chen, Ruey-Cheng},
    editor = {Kuribayashi, Tatsuki  and
      Rambelli, Giulia  and
      Takmaz, Ece  and
      Wicke, Philipp  and
      Oseki, Yohei},
    booktitle = {Proceedings of the Workshop on Cognitive Modeling and Computational Linguistics},
    month = {aug},
    year = {2024},
    address = {Bangkok, Thailand},
    publisher = {Association for Computational Linguistics},
    url = {https://aclanthology.org/2024.cmcl-1.1},
    doi = {10.18653/v1/2024.cmcl-1.1},
    pages = {1--7},
    abstract = {Children from bilingual backgrounds benefit from interactions with parents and teachers to re-acquire their heritage language. In this paper, we investigate how this insight from behavioral study can be incorporated into the learning of small-scale language models. We introduce BAMBINO-LM, a continual pre-training strategy for BabyLM that uses a novel combination of alternation and PPO-based perplexity reward induced from a parent Italian model. Upon evaluation on zero-shot classification tasks for English and Italian, BAMBINO-LM improves the Italian language capability of a BabyLM baseline. Our ablation analysis demonstrates that employing both the alternation strategy and PPO-based modeling is key to this effectiveness gain. We also show that, as a side effect, the proposed method leads to a similar degradation in L1 effectiveness as human children would have had in an equivalent learning scenario. Through its modeling and findings, BAMBINO-LM makes a focused contribution to the pre-training of small-scale language models by first developing a human-inspired strategy for pre-training and then showing that it results in behaviours similar to that of humans.},
}

@article{deldari2024auditnetconversationalaibasedsecurity,
      title={AuditNet: A Conversational AI-based Security Assistant [DEMO]}, 
      author={Shohreh Deldari and Mohammad Goudarzi and Aditya Joshi and Arash Shaghaghi and Simon Finn and Flora D. Salim and Sanjay Jha},
      year={2024},
      eprint={2407.14116},
      archivePrefix={arXiv},
      primaryClass={cs.CR},
      url={https://arxiv.org/abs/2407.14116}, 
      type={preprint}
}

@article{srirag2024predictingtargetwordgameplaying,
      title={Predicting the Target Word of Game-playing Conversations using a Low-Rank Dialect Adapter for Decoder Models}, 
      author={Dipankar Srirag and Aditya Joshi and Jacob Eisenstein},
      month = {September},
      year={2024},
      eprint={2409.00358},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2409.00358}, 
      type={preprint}
}